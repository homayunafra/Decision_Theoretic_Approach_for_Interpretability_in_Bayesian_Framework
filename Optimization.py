import tigraphs as tig
import numpy as np
import igraph as ig
from copy import deepcopy
from itertools import combinations
from math import log, floor, sqrt
from sklearn.model_selection import KFold

class DecisionNode(tig.BasicNode, object):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.pivot=None
        self.split_attribute=None
        self.left=None
        self.right=None
        self.children=None
        self.parent=None
        self.size=0
        self.depth=0
        self.local_data=None
        self.mu=None
        self.prob_vec=list()
        self.predicted_class=None

    # assign data points to a newly generated node.
    def local_filter(self, data):
        if self.parent == None:
            self.size = len(data)
            return data
        attribute = self.parent.split_attribute
        pivot = self.parent.pivot
        if type(pivot) == set:
            ret = data[attribute].isin(pivot)
        else:
            ret = data[attribute] <= pivot
        if self == self.parent.left:
            ret = data[ret]
        else:
            ret = data[~ret]
        self.size = len(ret)
        return ret

    # find the leaf to which the datapoint belongs
    def get_data_leaf(self, datapoint):
        if self.children == None:
            return self
        else:
            if type(self.pivot) == set:
                if datapoint[self.split_attribute] in self.pivot:
                    return self.left.get_data_leaf(datapoint)
                else:
                    return self.right.get_data_leaf(datapoint)
            else:
                if datapoint[self.split_attribute] <= self.pivot:
                    return self.left.get_data_leaf(datapoint)
                else:
                    return self.right.get_data_leaf(datapoint)

class DecisionTree(tig.return_tree_class(directed=True), object):
    def __init__(self, Vertex = DecisionNode, **kwargs):
        super().__init__(N=2, Vertex=Vertex, **kwargs)
        # these are default, can be set by train
        self.data_type_dict = {}
        self.tree_kind = None
        self.min_node_size = 1
        self.max_node_depth = 5
        self.data = None
        self.data_size = 0
        self.response = ''  # response attribute

    def fuse_vertex(self, vertex):
        super().fuse_vertex(vertex)
        vertex.left, vertex.right = None, None
        vertex.pivot, vertex.split_attribute = None, None

    # construct the tree using training data and an empty root vertex
    def grow_tree(self, ref):
        self.data_size = len(self.data)
        self.create_vertex()
        self.set_root(self.vertices[0])
        self.leaves.add(self.vertices[0])
        self.grow_node(self.get_root(), ref)

    # grow each node until reaching the stopping condition
    def grow_node(self, node, ref):
        if node.parent is None:
            node.local_data = node.local_filter(data=self.data)
            if self.tree_kind is 'regression':
                self.set_node_mu_ml(node, ref)
            else:
                self.set_node_prob_vec(node)
        if self.stopping_condition(node):
            return
        else:
            try:
                # select the best split (the one with minimum negative log-likelihood score)
                best_split = min(self.iter_split_eval(node, ref), key=lambda x: x[0])
                # no split has been selected yet, so fuse the node to erase the last split generated by iter_split()
                self.fuse_vertex(node)
            except ValueError:
                return
            if self.tree_kind is 'regression':
                ''' compute the MLE estimate of the variance parameter of the tree before splitting any internal nodes.
                check Step 1 of optimization algorithm in Section 3.2.1 of the paper '''
                sigma = sum(self.get_node_sse(leaf, ref) for leaf in self.leaves) / self.data_size
                base_impurity = node.size * log(sigma)
                if base_impurity - best_split[0] <= 0:
                    return
                # split the node based on the best split
                self.split_vertex(node, split_attribute=best_split[1], pivot=best_split[2])
                for child in node.children:
                    child.local_data = child.local_filter(data=node.local_data)
                    self.set_node_mu_ml(child, ref)
            else:
                value_count = self.data[self.response].value_counts()
                node_class_ll = [value_count[key] * log(value_count[key] / node.size) for key in value_count.keys()]
                node_neg_ll = -sum(node_class_ll)
                if node_neg_ll - best_split[0] <= 0:
                    return
                self.split_vertex(node, split_attribute=best_split[1], pivot=best_split[2])
                for child in node.children:
                    child.local_data = child.local_filter(data=node.local_data)
                    self.set_node_prob_vec(node)
            self.grow_node(node.left, ref)
            self.grow_node(node.right, ref)

    # prune the fully grown tree to avoid overfitting
    def prune_tree(self, alpha, tst_data, ref, run, m_ind, output_path):
        if alpha is None:
            ''' if alpha was none, we use cross-validation to construct a sequence of subtrees with their corresponding
            alpha values, from which the best subtree will be selected as the pruned tree'''
            subtree_ind = 1
            subtree_seq = {}
            alpha_seq = {}
            while True:
                try:
                    best_alpha, best_node = self.get_best_prune(ref)
                    self.fuse_vertex(best_node)
                    alpha_seq[subtree_ind] = best_alpha
                    subtree_seq[subtree_ind] = deepcopy(self)
                    subtree_ind += 1
                except ValueError:
                    break
            best_subtree_ind = self.cross_validate(alpha_seq, ref)
            return subtree_seq[best_subtree_ind+1]
        elif alpha == 0:
            ''' when alpha = 0, the algorithm prunes internal nodes one by one based on their prune cost with the goal
            to construct the acc vs complexity plots (check Fig. 4 in the paper)'''
            best_cost, best_node = self.get_best_prune(ref, alpha)
            self.fuse_vertex(best_node)
            treePerf = self.test(tst_data)
            treeSize = len(self.leaves)
            filename = output_path + "/result_proxy_" + str(m_ind) + "_" + str(run) + ".txt"
            with open(filename, 'a+') as outputfile:
                outputfile.write(str(treePerf) + "\t" + str(treeSize) + "\n")
            outputfile.close()
            if len(self.vertices) <= 3:
                return
            self.prune_tree(alpha, tst_data, ref, run, m_ind, output_path)

    # 5-fold cross-validation for selecting the best \alpha and it's corresponding subtree
    def cross_validate(self, alpha_seq_orig, ref):
        # beta_seq = list()
        beta_seq = [sqrt(alpha_seq_orig[i] * alpha_seq_orig[i + 1]) for i in range(1, len(alpha_seq_orig))]
        # for i in range(1, len(alpha_seq_orig)):
        #     beta_seq.append(sqrt(alpha_seq_orig[i] * alpha_seq_orig[i + 1]))
        beta_seq.append(100)
        cv_perf = np.full((5, len(beta_seq)), 10.5)
        cv = KFold(n_splits=5, random_state=42, shuffle=False)
        y = self.data[self.response]
        X = self.data.drop(self.response, axis=1)
        fold_ind = 0
        for train_index, test_index in cv.split(X):
            X_train, X_val, y_train, y_val = X.loc[train_index], X.loc[test_index], y.loc[train_index], y.loc[
                test_index]
            X_train.loc[:, self.response] = y_train
            X_val.loc[:, self.response] = y_val
            if self.tree_kind is 'regression':
                tree = RegressionTree()
                tree.tree_kind = 'regression'
            else:
                tree = ClassificationTree()
                tree.tree_kind = 'classification'
            tree.data = X_train
            tree.data_type_dict = self.data_type_dict
            tree.response = self.response
            tree.min_node_size = self.min_node_size
            tree.max_node_depth = self.max_node_depth
            tree.grow_tree(ref)
            best_j = 0
            while True:
                try:
                    best_alpha, best_node = tree.get_best_prune(ref)
                    tree.fuse_vertex(best_node)
                    subtree_acc = tree.test(X_val)
                    j = min([index for index, value in enumerate(beta_seq) if best_alpha <= value])
                    if j == best_j:
                        cv_perf[fold_ind][j] = min(subtree_acc, cv_perf[fold_ind][j])
                    else:
                        cv_perf[fold_ind][j] = subtree_acc
                    best_j = j
                except ValueError:
                    break
            fold_ind += 1
        min_index = np.argmin(np.mean(cv_perf, axis=0))
        return min_index

    # compute the negative log-likelihood score of each split
    def iter_split_eval(self, node, ref):
        for split in self.iter_split(node):
            if node.children is None:
                pass
            else:
                for child in node.children:
                    child.local_data = child.local_filter(node.local_data)
                if node.left.size < self.min_node_size or node.right.size < self.min_node_size:
                    self.fuse_vertex(node)
                    continue
                # compute the maximum likelihood estimate of the variance parameter of tree given the current split
                if self.tree_kind is 'regression':
                    for child in node.children:
                        self.set_node_mu_ml(child, ref)
                    sigma = sum(self.get_node_sse(leaf, ref) for leaf in self.leaves) / self.data_size
                    left_impurity = node.left.size * log(sigma)
                    right_impurity = node.right.size * log(sigma)
                    split_impurity = left_impurity + right_impurity
                else:
                    for child in node.children:
                        self.set_node_prob_vec(child)
                    value_count = self.data[self.response].value_counts()
                    class_ll_left = [value_count[key]*log(value_count[key] / node.left.size) for key in value_count.keys()]
                    left_purity = sum(class_ll_left)
                    class_ll_right = [value_count[key]*log(value_count[key] / node.right.size) for key in value_count.keys()]
                    right_purity = sum(class_ll_right)
                    split_impurity = -(left_purity + right_purity)
                ret = [split_impurity, node.split_attribute, node.pivot]
                yield ret

    # access to all possible splits
    def iter_split(self, node):
        for attribute in self.data.columns:
            if attribute != self.response and attribute != 'predictive_var':
                for pivot in self.get_pivots(node.local_data, attribute):
                    self.fuse_vertex(node)
                    self.split_vertex(vertex=node, pivot=pivot,
                                      split_attribute=attribute)
                    yield

    # split the given vertex based on the given pivot and attribute
    def split_vertex(self, vertex, split_attribute, pivot):
        super(DecisionTree, self).split_vertex(vertex)
        vertex.left = vertex.children[0]
        vertex.left.depth = vertex.depth + 1
        vertex.right = vertex.children[1]
        vertex.right.depth = vertex.depth + 1
        vertex.pivot, vertex.split_attribute = pivot, split_attribute

    # accessing to all possible pivots of a given attribute
    def get_pivots(self, data, attribute):
        if self.data_type_dict[attribute] == 'ordinal':
            max_pivot = max(data[attribute].unique())
            for pivot in data[attribute].unique():
                if pivot < max_pivot:
                    yield pivot
        elif self.data_type_dict[attribute] == 'nominal':
            values = data[attribute].unique()
            n = len(values)
            if n <= 1:
                return
            n = floor(float(n) / 2)
            n = int(n)
            for r in range(1, n + 1):
                for pivot in combinations(values, r):
                    yield set(pivot)

    def stopping_condition(self, node):
        if self.max_node_depth <= node.depth:
            return True
        elif node.size <= self.min_node_size:
            return True
        else:
            return False

    def get_node_entropy(self, node):
        def entropy_summand(node, p):
            if p == 0:
                return 0
            else:
                return float(len(node.local_data)) * p * log(p) # equivalent to n_k * log(p)
        return -sum(entropy_summand(node, p) for p in node.prob_vec)

    # compute the sum-of-squared error for the current node (for regression trees)
    def get_node_sse(self, node, ref):
        if not ref:
            return sum((y - node.mu) ** 2 for y in node.local_data[self.response])
        else:
            return sum(sample['predictive_var'] + (sample[self.response] - node.mu) ** 2 for
                key, sample in node.local_data.iterrows())

    # compute maximum likelihood estimate of the mean parameter of the current node (for regression trees)
    def set_node_mu_ml(self, node, ref):
        if ref:
            node.mu = sum(sample[self.response] for key, sample in node.local_data.iterrows()) / node.size
        else:
            node.mu = node.local_data[self.response].mean()

    # compute the maximum likelihood estimate of the probability vector of the current node (for classification trees)
    def set_node_prob_vec(self, node):
        size = float(len(node.local_data))
        value_count = node.local_data[self.response].value_counts()
        node.prob_vec = [value_count[key] / size for key in value_count.keys()]

    # returns the best internal node to be prunned with the pruning cost
    def get_best_prune(self, ref, alpha=None):
        best_cut_cost, best_node = min(self.iter_prune_cost(ref, alpha),
                                            key=lambda x: x[0])
        return [best_cut_cost, best_node]

    # computes the pruning costs of each internal node
    def iter_prune_cost(self, ref, alpha):
        for node in self.vertices:
            if node not in self.leaves and node is not self.get_root():
                yield [self.prune_cost(node, ref, alpha), node]

    def prune_cost(self, node, ref, alpha):
        if self.tree_kind is 'regression':
            node_cost = 0
            for leaf in self.leaves:
                if leaf not in self.get_node_leaves(node):
                    node_cost += self.get_node_sse(leaf, ref)
            node_cost += self.get_node_sse(node, ref)
            if alpha is None:
                node_cost = log(node_cost)
                subtree_cost = log(sum(self.get_node_sse(leaf, ref) for leaf in self.leaves))
                return (node_cost - subtree_cost) / (len(self.get_node_leaves(node))-1)
            elif alpha == 0:
                # The goal is to minimize log(\sigma^2) with \sigma^2 = \sum_{i=1^b}\sum(j=1^n_i) SSE_i. This is done by
                # selecting a node to be prunned which minimizes C(T_t) where T_t is a subtree generated by prunning the
                # internal node t from the tree T.
                return log(node_cost)
        else:
            node_cost = 0
            for leaf in self.leaves:
                if leaf not in self.get_node_leaves(node):
                    node_cost += self.get_node_entropy(leaf)
            node_cost += self.get_node_entropy(node)
            if alpha is None:
                subtree_cost = sum(self.get_node_entropy(leaf) for leaf in self.leaves)
                return (node_cost - subtree_cost) / (len(self.get_node_leaves(node))-1)
            elif alpha == 0:
                return node_cost
            else:
                return node_cost + alpha * (len(self.leaves) - len(self.get_node_leaves(node)) + 1)

    # assign the new data to the leaves
    def load_new_data(self, data):
        self.data = data
        self.data_size = len(data)
        for node in self.node_iter_down(self.get_root()):
            if node.parent == None:
                node.local_data = node.local_filter(data)
            else:
                node.local_data = node.local_filter(node.parent.local_data)

    # fetching the leaves of a subtree rooted at node
    def get_node_leaves(self, node):
        leaves = set([])
        for descendant in self.node_iter_down(node):
            if descendant in self.leaves:
                leaves.add(descendant)
        return leaves

    # traversing a subtree rooted at base to its leaves
    def node_iter_down(self, base, first=True):
        if first:
            yield base
            if base.children == None:
                return
        if base.children == None:
            yield base
        else:
            for child in base.children:
                yield child
                for node in self.node_iter_down(child, first=False):
                    yield node

    def train(self, tr_data, data_type_dict, parameters, output_path=None, tst_data=None, run=None, m_ind=None):
        ''' parameters:
        run: index of each run in [1,50],
        m_ind: method index: 0 refers to the interpretability prior approach (when fitting proxy model to the original
                training data; 1-3 refers to the proxy model fitted to the reference model where:
                1: BART with ntree obtained using CV
                2: BART with ntree = 3, and
                3: GP.'''
        self.vertices = []
        self.edges = set([])
        self.leaves = set([])
        self.data_type_dict = data_type_dict
        self.tree_kind = parameters['tree_kind']
        self.response = parameters['response']
        self.min_node_size = parameters['min_node_size']
        self.max_node_depth = parameters['max_node_depth']
        alpha = parameters['alpha']
        prune = parameters['prune']
        ref = parameters['ref']
        self.data = tr_data
        self.grow_tree(ref)
        if prune:
            if alpha is None:
                return self.prune_tree(alpha, tst_data, ref, run, m_ind, output_path)
            else:
                self.prune_tree(alpha, tst_data, ref, run, m_ind, output_path)

    # fetching all the data of a leaf node
    def get_data_leaf(self, datapoint):
        if self.children is None:
            return self
        else:
            if type(self.pivot) == set:
                if datapoint[self.split_attribute] in self.pivot:
                    return self.left.get_data_leaf(datapoint)
                else:
                    return self.right.get_data_leaf(datapoint)
            else:
                if datapoint[self.split_attribute] <= self.pivot:
                    return self.left.get_data_leaf(datapoint)
                else:
                    return self.right.get_data_leaf(datapoint)

    # find the leaf node to which newdata belongs and send back the mean parameter of the leaf
    def predict(self, newdata):
        predictions = np.zeros(newdata.shape[0])
        for i, (index, datapoint) in enumerate(newdata.iterrows()):
            if self.tree_kind is 'regression':
                predictions[i] = self.vertices[0].get_data_leaf(datapoint).mu
            else:
                probs = self.vertices[0].get_data_leaf(datapoint).prob_vec
                predictions[i] = probs.index(max(probs))
        return predictions

    # compute the RMSE of the test data
    def test(self, data):
        if self.tree_kind is 'regression':
            pred = self.predict(data)
            return sqrt(sum((data[self.response] - pred) ** 2)/len(data))
        else:
            miss_classification_rate = 0
            for leaf in self.leaves:
                leaf.predicted_class = leaf.prob_vec.index(max(leaf.prob_vec))
                leaf_error = leaf.local_data[self.response].value_counts()
                if leaf.predicted_class in leaf_error.keys():
                    leaf_error = 1 - leaf_error[leaf.predicted_class]/float(len(leaf.local_data))
                else:
                    leaf_error = 1
                miss_classification_rate += leaf_error
            return miss_classification_rate

class RegressionTree(DecisionTree, object):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def plot(self, s = 0, margin = 35):
        A = self.get_adjacency_matrix_as_list()
        convert_to_igraph = ig.Graph.Adjacency(A)
        g = convert_to_igraph
        for vertex in self.vertices:
            index = self.vertices.index(vertex)
            if vertex.pivot != None:
                if type(vertex.pivot) == set:
                    label_pivot = ' in ' + str(list(vertex.pivot))

                else:
                    label_pivot = ' less than ' + str(round(vertex.pivot, 2))
                g.vs[index]['label'] = str(vertex.split_attribute) + label_pivot
                g.vs[index]['label_dist'] = 3
                g.vs[index]['label_color'] = 'red'
                g.vs[index]['color'] = 'red'
                g.vs[index]['size'] = 40
            else:
                label = round(vertex.mu, 2)
                g.vs[index]['color'] = 'blue'
                g.vs[index]['label'] = str(label)
                g.vs[index]['label_dist'] = 1
                g.vs[index]['label_color'] = 'blue'
                g.vs[index]['size'] = 40
        root_index = self.vertices.index(self.get_root())
        layout = g.layout_reingold_tilford(root=[root_index])
        ig.plot(g, layout=layout, margin=margin)

class ClassificationTree(DecisionTree, object):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def plot(self, s=0, margin=35):
        A = self.get_adjacency_matrix_as_list()
        convert_to_igraph = ig.Graph.Adjacency(A)
        g = convert_to_igraph
        for vertex in self.vertices:
            index = self.vertices.index(vertex)
            if vertex.pivot != None:

                if type(vertex.pivot) == set:
                    label_pivot = ' in ' + str(list(vertex.pivot))

                else:
                    label_pivot = ' less than ' + str(vertex.pivot)
                g.vs[index]['label'] = str(vertex.split_attribute) + label_pivot
                g.vs[index]['label_dist'] = 3
                g.vs[index]['label_color'] = 'red'
                g.vs[index]['color'] = 'red'
                g.vs[index]['size'] = 40

            else:
                label = str(vertex.predicted_class)
                g.vs[index]['color'] = 'blue'
                g.vs[index]['label'] = label
                g.vs[index]['label_dist'] = 1
                g.vs[index]['label_color'] = 'blue'
                g.vs[index]['size'] = 40

        root_index = self.vertices.index(self.get_root())
        layout = g.layout_reingold_tilford(root=root_index)
        ig.plot(g,  "example_tree_" + str(s) + ".pdf", layout=layout, bbox=(650, 650), margin=margin)

